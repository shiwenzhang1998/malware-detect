from sklearn.model_selection import train_test_split
import os
import pandas as pd
import numpy as np
from tqdm import tqdm
from pathlib import Path
from copy import deepcopy

def byte_transform(labelpath,datapath):
    label=pd.read_csv(labelpath)

    files = os.listdir(datapath)
    file_basenames = label['Id'].tolist()
    class_labels = label['Class'].tolist()
    class_bytes = []
    sizebytes = []
    fnames = []
    for file in tqdm(files):
        if not file.endswith("bytes"):
            continue

        file_path = os.path.join(datapath,file)
        statinfo = os.stat(file_path)
        filename = Path(file).stem
        if any(filename == file_basename for file_basename in file_basenames):
            i = file_basenames.index(filename)
            class_bytes.append(class_labels[i])
            sizebytes.append(statinfo.st_size / (1024.0 * 1024.0))
            fnames.append(filename)

        txt_path = os.path.join(datapath,filename,".txt")
        txt_file = open(txt_path, 'w+')
        with open(file_path, "r") as F:
            for line in F:
                a = line.rstrip().split(" ")[1:]
                b = ' '.join(a)
                b = b + "\n"
                txt_file.write(b)
            F.close()
        txt_file.close()

    size_feature = pd.DataFrame({'ID': fnames, 'size': sizebytes, 'Class': class_bytes})
    size_feature.to_csv("size_feature.csv",header=True,index=False)
    return size_feature

def save_features(datapath):
    byte_feature = open('byte_feature.csv', 'w+')
    byte_feature.write(
        "ID,0,1,2,3,4,5,6,7,8,9,0a,0b,0c,0d,0e,0f,10,11,12,13,14,15,16,17,18,"
        "19,1a,1b,1c,1d,1e,1f,20,21,22,23,24,25,26,27,28,29,2a,2b,2c,2d,2e,2f,"
        "30,31,32,33,34,35,36,37,38,39,3a,3b,3c,3d,3e,3f,40,41,42,43,44,45,46,"
        "47,48,49,4a,4b,4c,4d,4e,4f,50,51,52,53,54,55,56,57,58,59,5a,5b,5c,5d,"
        "5e,5f,60,61,62,63,64,65,66,67,68,69,6a,6b,6c,6d,6e,6f,70,71,72,73,74,"
        "75,76,77,78,79,7a,7b,7c,7d,7e,7f,80,81,82,83,84,85,86,87,88,89,8a,8b,"
        "8c,8d,8e,8f,90,91,92,93,94,95,96,97,98,99,9a,9b,9c,9d,9e,9f,a0,a1,a2,"
        "a3,a4,a5,a6,a7,a8,a9,aa,ab,ac,ad,ae,af,b0,b1,b2,b3,b4,b5,b6,b7,b8,b9,"
        "ba,bb,bc,bd,be,bf,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,ca,cb,cc,cd,ce,cf,d0,"
        "d1,d2,d3,d4,d5,d6,d7,d8,d9,da,db,dc,dd,de,df,e0,e1,e2,e3,e4,e5,e6,e7,"
        "e8,e9,ea,eb,ec,ed,ee,ef,f0,f1,f2,f3,f4,f5,f6,f7,f8,f9,fa,fb,fc,fd,fe,"
        "ff,??\n")

    files = os.listdir(datapath)
    feature_mat = np.zeros((len(files), 257), dtype=int)
    index_ = 0
    for file in tqdm(files):
        if not file.endswith("txt"):
            continue
        byte_feature.write(file + ",")
        file_path = os.path.join(datapath, file)

        with open(file_path, "r") as byte_flie:
            for lines in byte_flie:
                line = lines.rstrip().split(" ")
                for hex_code in line:
                    if hex_code == '??':
                        feature_mat[index_][256] += 1
                    else:
                        feature_mat[index_][int(hex_code, 16)] += 1
        byte_flie.close()

        for i, row in enumerate(feature_mat[index_]):
            if i != len(feature_mat[index_]) - 1:
                byte_feature.write(str(row) + ",")
            else:
                byte_feature.write(str(row))

        byte_feature.write("\n")

        index_ += 1

    byte_feature.close()

def normalize(df):
    result = deepcopy(df)
    for feature_name in df.columns:
        if (str(feature_name) != str('ID') and str(feature_name) != str('Class')):
            max_value = df[feature_name].max()
            min_value = df[feature_name].min()
            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)
    return result

def get_data(size_feature_path,byte_feature_path):
    size_feature = pd.read_csv(size_feature_path)
    byte_feature = pd.read_csv(byte_feature_path)
    byte_feature['ID'] = byte_feature['ID'].str.split('.').str[0]
    byte_feature.head(2)
    size_feature.head(2)
    merge_feature = byte_feature.merge(size_feature, on='ID')
    merge_feature.to_csv("merge_feature.csv")
    merge_feature.head(2)

    result = normalize(merge_feature)
    result.head(2)

    label = result['Class']
    X_train, X_test, y_train, y_test = train_test_split(result.drop(['ID', 'Class'], axis=1), label, stratify=label,test_size=0.20)
    return X_train, X_test, y_train, y_test


