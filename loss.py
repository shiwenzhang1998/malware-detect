import torch
import torch.nn as nn
import numpy as np

device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')

class CLSloss(nn.Module):
    def __init__(self):
        super(CLSloss, self).__init__()
        self.base_loss = nn.functional.cross_entropy

        # parameters for focal-loss
        self.alpha = 1
        self.beta = 2

        # parameters for labelsmooth-loss
        self.smoothing = 0.1
        self.confidence = 1.0 - self.smoothing

    def weighted_cross_entropy(self,output, target, weight=1.5):
        # if target label in weighted_class[list],increase its weight.
        self.weighted_class = [0, 9, 10]
        loss = self.base_loss(output, target, reduction="none")
        target = target.cpu().numpy()
        weights = np.ones((target.shape))
        for id, label in enumerate(target):
            if label in self.weighted_class:
                weights[id] = weight
        weights = torch.from_numpy(weights).to(device, dtype=torch.float32)
        loss *= weights
        return loss

    def focal_loss(self,output,target):
        # focal loss
        # use to mine difficult case
        loss = self.base_loss(output, target, reduction="none")
        weight = torch.exp(loss)
        return (self.alpha*(1-weight)**self.beta*loss).mean()

    def label_smooth_loss(self, output, target):
        # reduce overfitting
        # add smooth_loss on the basis of bce_loss
        output_probs = torch.nn.functional.log_softmax(output, dim=-1)
        bce_loss = -output_probs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)
        smooth_loss = -output_probs.mean(dim=-1)
        loss = self.confidence * bce_loss + self.smoothing * smooth_loss
        return loss.mean()

    def forward(self, output, target):
        return self.label_smooth_loss(output,target)